\documentclass[../../main/main.tex]{subfiles}

\begin{document}
    \begin{figure*}[hp]
        \makebox[\textwidth]{\subfile{./1_tree/tree.tex}}
        \caption{Anomaly detection method categorization}
    \end{figure*}

    Anomaly detection method can be categorize in many way. In this paper its divided in two parts,
    general method and useful ticks to improve theses. The former represent general ways to 
    solve the AD problem. Its compose of tree categorizes : forecasting, clustering and index monitoring.
    The latter are so called add-ons. The papers in this category present a AD method with some tricks which,
    in our mind can be applied for any method to improve it.
    \subsection{Forecasting Methods}
        When working all day long with some equipment, you know it so well that if asked you could almost 
        predict the sound it will make in some close future. But if suddenly sound doesn't match 
        your expectation  you know there is some things wrong.
        That is the core idea for the forecasting method, predict the near future using some generative method
        and if the error go over some threshold the data point is labeled as an anomaly. Example of those technic 
        may be LSTM [ref claim LSTM is the best] or using time convolution (TCN) [Ref TCN].
    
    \subsection{Clustering method}
        While storing metric screw in the ware house some one pick an imperial one, he want find any box to put it in.
        Therefor he will conclude a error in the shipment, even if he didn't know this type of screw. This is the main 
        idea for the clustering method, define clusters, if a new data point can't fit in one of them it labeled as an anomaly.  
        There is multiple way to define the cluster. A good part use some define distance [ref DTW] function and use a variant of K-nearest-neighbor (KNN)
        [ref DBSCAN]. In a similar fashion use the density of the data point in relation with the training data which will 
        give less strick categorizes [ref LOF]. An other method is to estimate the probability density function (PDF) 
        and put a threshold on low values. [ref DAGMM]

    \subsection{Index monitoring}
        After assembly, of the disks cutters there are put under pressure for some time. When the time is up, the 
        pressure is measured, if the pressure is to low there is a problem some where. This is the intuition behind index 
        monitoring method, define some function with an acceptable range of normal values, if the values is outside the range 
        its mark as anomaly. This technic is one of the oldest, simplest and most use in the manufacturing industries, in
        this context are referred as Control chart [ref Control chart]. An other example of application is [ref AR] which 
        use a modify wavelet transform to define a Health Index [ref HI] to monitor bearings. 

    \subsection{Add-ones}
        In order improve those technics and solve issue intrinsic to the TBM case here are some interesting solutions.\\
        \textit{Topology}, TBM AD is intrinsically a multivariate problem (ie their is more than one sensor on the machine).
        And it is safe to assume that the anomaly information for some problem is carry by multiple sensors. For example if there 
        is a defective sensor, the others wan't raise any anomaly which will greatly help to pin the root cause down. This what 
        [ref GDN] and [ref GTA] did by introducing a directed graph to model relationship between sensors. This information 
        was then use, in this papers, to do forecasting using attention technics.\\
        
        \textit{Contamination}, the training data for those model will surely be some already bore tunnel data. 
        During those bore, by Murphy's law, at some point, some things went wrong. The issue is, most method assume a anomaly
        free dataset. This is why [ref IAD, LOE, Core Loss] proposed some modification to the training steps, to deal with this 
        contamination. To do so, they generally modify the loss function to account for the uncertainty of the data and improve 
        this uncertainty iteratively. This topic will be discus in greater detail in \ref{sect:contamination}.\\

        \textit{Dimension reduction}, a large quantity of sensors, with a quite small data rate, even with a small time window
        can make the input dimension of the model certainly significant. This can render the training hard and sensible to noise. 
        to solve this issue most paper use some kind of dimension reduction. For example, [ref DAGMM] use a AE and use
        a modify reconstruction error concatenated with the latent vector as an input to a GMM. Or [LSTM-AE] used a LSTM as
        an AE to do forecasting.


        
    %\subfile{./plan.tex}
    
\end{document}